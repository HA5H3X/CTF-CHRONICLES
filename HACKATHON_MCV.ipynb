{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1MepiXhhJ8Flc2UEgOozLP5kO7S_a1NfH",
      "authorship_tag": "ABX9TyPQzltUZWLSqU9SPWYnO6ww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/W4W1R3/CTF-CHRONICLES/blob/main/HACKATHON_MCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBFNUgptgBgF",
        "outputId": "0c3174a7-8b58-4244-935e-930f20108804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.13.0\n",
            "GPU Available: []\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "# For data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# For audio processing\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "# For deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.experimental.list_physical_devices('GPU')}\")\n",
        "\n",
        "# If you're using TensorFlow, make sure to check if GPU support is available.\n",
        "# If a GPU is available, TensorFlow will utilize it for faster training.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Define the paths to the tar.gz files in Google Drive\n",
        "train_tar_gz = '/content/drive/My Drive/HACKATHON-MCV/train0.tar.gz'\n",
        "\n",
        "# Define the destination directories for extraction\n",
        "destination_train = '/content/drive/My Drive/HACKATHON-MCV/train/'\n",
        "\n",
        "# Create the destination directories if they don't exist\n",
        "if not os.path.exists(destination_train):\n",
        "    os.makedirs(destination_train)\n",
        "\n",
        "\n",
        "# Extract the dataset\n",
        "def extract_tar_gz(tar_gz_file, destination_directory):\n",
        "    with tarfile.open(tar_gz_file, 'r:gz') as tar:\n",
        "        tar.extractall(path=destination_directory)\n",
        "\n",
        "# Extract the train dataset\n",
        "extract_tar_gz(train_tar_gz, destination_train)\n",
        "\n",
        "\n",
        "\n",
        "# List the extracted files\n",
        "extracted_train_files = os.listdir(destination_train)\n"
      ],
      "metadata": {
        "id": "VV4pvRqzkkFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Loading: Load Your Training Dataset\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory where your dataset is extracted\n",
        "dataset_directory = '/content/drive/My Drive/HACKATHON-MCV/train/train'  # Replace with your actual directory\n",
        "\n",
        "# List all the files in the dataset directory\n",
        "files = os.listdir(dataset_directory)\n",
        "\n",
        "# Load the data (assuming it's in CSV format, adjust for other formats)\n",
        "dataframes = []\n",
        "\n",
        "for file in files:\n",
        "    if file.endswith('.csv'):\n",
        "        file_path = os.path.join(dataset_directory, file)\n",
        "        df = pd.read_csv(file_path)\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes into one if necessary\n",
        "if dataframes:\n",
        "    training_data = pd.concat(dataframes)\n",
        "\n",
        "# Now you have your training dataset loaded in the 'training_data' variable\n"
      ],
      "metadata": {
        "id": "H3xl7FgemB_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the ASR model architecture\n",
        "audio_feature_shape = (40, 13)  # Input shape for audio features\n",
        "vocabulary_size = 30000  # Replace with the actual vocabulary size of your dataset\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # Recurrent layers for sequence modeling\n",
        "    layers.Bidirectional(layers.LSTM(256, return_sequences=True), input_shape=audio_feature_shape),\n",
        "    layers.Bidirectional(layers.LSTM(256, return_sequences=True)),\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(vocabulary_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with loss and optimizer\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uznh8N0W2Ofn",
        "outputId": "b0a43c39-2499-4976-b4ca-b8aa1b7bbd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 40, 512)           552960    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 40, 512)           1574912   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 40, 30000)         15390000  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17517872 (66.83 MB)\n",
            "Trainable params: 17517872 (66.83 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example audio features (replace with your actual data)\n",
        "train_data = np.random.randn(100, 40, 13)  # 100 samples, 40 time steps, 13 feature dimensions\n",
        "val_data = np.random.randn(20, 40, 13)    # 20 samples for validation\n"
      ],
      "metadata": {
        "id": "DV_HBJYAI3XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example transcriptions (replace with your actual labels)\n",
        "train_labels = \"/content/drive/My Drive/HACKATHON-MCV/train/\"  # List of transcription strings\n",
        "val_labels =  \"/content/drive/My Drive/HACKATHON-MCV/eval/\"  # Validation transcriptions\n"
      ],
      "metadata": {
        "id": "ETNsqJ9iI5ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "_gBsYjn1ow-G",
        "outputId": "b584f442-86ce-40e6-b4a5-a309d3ea14dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1419cc11b62f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load your training data and labels as NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Replace these paths with your actual data and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/HACKATHON-MCV/train_data.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/HACKATHON-MCV/train_labels.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/HACKATHON-MCV/train_data.npy'"
          ]
        }
      ]
    }
  ]
}